{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python | Pytorch | OpenCV versions: 3.7.0 (default, Jun 28 2018, 13:15:42) \n",
      "[GCC 7.2.0] | 0.4.1 | 3.4.3\n",
      "lol\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import sys\n",
    "from importlib import reload\n",
    "import cv2\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "print('Python | Pytorch | OpenCV versions: %s | %s | %s' %(sys.version, torch.__version__, cv2.__version__))\n",
    "\n",
    "sys.path.append('/home/blanca/mnt/projects/')\n",
    "import tools\n",
    "from tools.utils import *\n",
    "from tools.visuals import *\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"lol\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "741\n",
      "641\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1382"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xml_annotations_folder_path = '__toy_plane/Annotations_folders'\n",
    "xml_annotations_folder_path = Path(xml_annotations_folder_path)\n",
    "\n",
    "ann_files = []\n",
    "# for xml_annotations_path in xml_annotations_folder_path.glob('*'):\n",
    "# #     if xml_annotations_path.name == 'annotations_2':\n",
    "#         print(len(list(xml_annotations_path.glob(\"*.xml\"))))\n",
    "#         folder_files = np.array(list(xml_annotations_path.glob(\"*.xml\")))\n",
    "#         ann_files.append(folder_files)\n",
    "\n",
    "xml_annotations_paths = [\n",
    "    '__toy_plane/9009_task_2',\n",
    "    '__toy_plane/9009_task_3'\n",
    "]\n",
    "for xml_annotations_path in xml_annotations_paths:\n",
    "    path = Path(xml_annotations_path)\n",
    "    print(len(list(path.glob(\"*.xml\"))))\n",
    "    ann_files.append(np.array(list(path.glob(\"*.xml\"))))\n",
    "\n",
    "ann_files = np.concatenate(ann_files, axis=0)\n",
    "len(ann_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# screened_images_path = Path('/home/blanca/mnt/data/__toy_plane/JPEGImages_9009_task_2_screened')\n",
    "# screened_images = list(screened_images_path.glob(\"*.jpg\"))\n",
    "# screened_images_names = [i.name.split('.jpg')[0] for i in screened_images]\n",
    "# len(screened_images_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = xml_annotations_folder_path\n",
    "cat_file = path.parent / Path('toy_plane_classes_attbs_body_drill.names') # 11 classes\n",
    "\n",
    "with open(str(cat_file), 'r') as f:\n",
    "    cat_data = [i.strip() for i in f.readlines()]\n",
    "\n",
    "cat_data = [i.lower() for i in cat_data]\n",
    "cat_dic = {i:cat_data[i] for i in range(len(cat_data))}\n",
    "cat_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [\n",
    "    'Body', \n",
    "    'Drill'\n",
    "]\n",
    "attbs_names = [\n",
    "    'Has wing 1', \n",
    "    'Has wing 2', \n",
    "    'Has small wheels', \n",
    "    'Has propeller', \n",
    "    'Has cockpit', \n",
    "    'Has carriage',\n",
    "    'Has drill tip', \n",
    "    'Has hex drill tip',\n",
    "    'Button pressed', \n",
    "]\n",
    "\n",
    "class_names = [i.lower() for i in class_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import xmltodict\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from skimage import io\n",
    "import operator\n",
    "from functools import reduce\n",
    "import collections\n",
    "\n",
    "def xy_xy2ulxy_wh(x):  # Convert bounding box format from [x1, y1, x2, y2] to [x, y, w, h]\n",
    "    y = np.zeros(len(x))\n",
    "    y[0] = x[0]\n",
    "    y[1] = x[1]\n",
    "    y[2] = x[2] - x[0]\n",
    "    y[3] = x[3] - x[1]\n",
    "    return y.tolist()\n",
    "\n",
    "def xy_xy2bbxy_wh(x):  # Convert bounding box format from [x1, y1, x2, y2] to [x, y, w, h]\n",
    "    y = np.zeros(len(x))\n",
    "    y[0] = (x[0] + x[2]) / 2\n",
    "    y[1] = (x[1] + x[3]) / 2\n",
    "    y[2] = x[2] - x[0]\n",
    "    y[3] = x[3] - x[1]\n",
    "    return y.tolist()\n",
    "     \n",
    "def adjust_polygon_format(o_polygon):\n",
    "    polygon_list = []\n",
    "    co_k_x = [i for i in list(o_polygon.keys()) if i[0] == 'x']\n",
    "    co_k_x = sorted(co_k_x, key=lambda x: int(x[1:]))\n",
    "    for i in co_k_x:\n",
    "        ix = i[1:]\n",
    "        xk, yk = i, 'y' + ix\n",
    "        polygon_list.append([float(o_polygon[xk]), float(o_polygon[yk])])\n",
    "        \n",
    "    polygon_list = reduce(operator.add, polygon_list)\n",
    "    \n",
    "    return polygon_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_attbs_dic = { # contains the folder name as key and a list of the missing attributes as the value\n",
    " 'annotations_10' : [True, True, True, True, False, True],\n",
    " 'annotations_11' : [True, True, False, False, True, True],\n",
    " 'annotations_12' : [True, True, False, True, True, True],\n",
    " 'annotations_13' : [True, True, True, True, True, False],\n",
    " 'annotations_14' : [True, True, True, False, True, True],\n",
    " 'annotations_2' : [True, True, True, True, True, True],\n",
    " 'annotations_3' : [False, True, True, False, True, False],\n",
    " 'annotations_4' : [True, False, True, True, True, True],\n",
    " 'annotations_5' : [True, False, True, True, False, True],\n",
    " 'annotations_6' : [True, False, True, True, True, False],\n",
    " 'annotations_7' : [False, False, True, True, False, True],\n",
    " 'annotations_8' : [False, True, True, True, True, True],\n",
    " 'annotations_9' : [False, False, False, True, False, False]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mcnn_dataset(ann_files, cat_file, att_file=None):\n",
    "\n",
    "    with open(str(cat_file), 'r') as f:\n",
    "        cat_data = [i.strip() for i in f.readlines()]\n",
    "        cat_data = [i.lower() for i in cat_data]\n",
    "\n",
    "    cat_dic = {i:cat_data[i] for i in range(len(cat_data))}\n",
    "\n",
    "    ### CATEGORIES ###\n",
    "\n",
    "    categories = [{\n",
    "        \"id\" : i, #int, \n",
    "        \"name\" : j, #str, \n",
    "        \"supercategory\" : j, #str,\n",
    "    } for i, j in cat_dic.items()]\n",
    "\n",
    "    ### DATASET ###\n",
    "\n",
    "    info = {\n",
    "        \"year\" : '2018', \n",
    "        \"version\" : '1', \n",
    "        \"description\" : 'fumehood', \n",
    "        \"contributor\" : 'v7', \n",
    "        \"url\" : 'none', \n",
    "        \"date_created\" : '2018/09/20',\n",
    "    }\n",
    "\n",
    "    dataset = {\n",
    "\n",
    "        \"info\" : info, \n",
    "        \"images\" : [],       # image\n",
    "        \"annotations\" : [],  # annotation\n",
    "        \"licenses\" : [],     # license\n",
    "        \"categories\": categories\n",
    "    }\n",
    "\n",
    "    count_i = 0\n",
    "    count_o = 0\n",
    "    total_dv_files = 0\n",
    "    \n",
    "    obss_wo_atts = 0\n",
    "    obss_w1_atts = 0\n",
    "    obss_ws_atts = 0\n",
    "    \n",
    "    default_anns = 0\n",
    "    count_not_anno = 0\n",
    "    \n",
    "    for file in ann_files:\n",
    "        task = file.parent.name\n",
    "        default_attbs = None\n",
    "        if task in list(default_attbs_dic.keys()):\n",
    "            default_attbs = default_attbs_dic[task]\n",
    "            default_attbs = [attbs_names[i] for i in range(len(default_attbs)) if default_attbs[i] == True]\n",
    "        \n",
    "        with open(str(file)) as fd:\n",
    "            try: doc = xmltodict.parse(fd.read())\n",
    "            except: print(file); continue\n",
    "            count_i += 1\n",
    "            # creating an image: \n",
    "            data = doc['annotation']\n",
    "            # image\n",
    "            folder = data['folder']\n",
    "            filename = data['filename']        \n",
    "            session_type = folder.split('/')[-2]\n",
    "            session = folder.split('/')[-1]\n",
    "            im_name = '{:06d}'.format(int(filename.split('.png')[0]))\n",
    "            im_name = '%s_%s_%s.png' %(session_type, session, im_name)\n",
    "            if not default_attbs: \n",
    "                if im_name.split('.png')[0] in screened_images_names:\n",
    "#                 ix_name = int(filename.split('.png')[0])\n",
    "#                 if ix_name < 189 or ix_name > 723:\n",
    "                    pre = task[:4]\n",
    "                    im_name = pre + '_' + im_name  \n",
    "#                     print(im_name)\n",
    "                else: continue\n",
    "                \n",
    "            url = folder + '/' + filename\n",
    "            \n",
    "            try:\n",
    "                width = int(data['size']['width'])\n",
    "                height = int(data['size']['height'])\n",
    "            except: continue\n",
    "            try: objects = data['object']\n",
    "            except: \n",
    "#                 print('HERE 2', file); \n",
    "                count_not_anno +=1; \n",
    "                continue\n",
    "\n",
    "            image_dic = {\n",
    "                \"id\" : count_i, #int, \n",
    "                \"width\" : width, #int, \n",
    "                \"height\" : height, #int, \n",
    "                \"file_name\" : im_name, #str, \n",
    "                \"license\" : 0, #int, \n",
    "                \"flickr_url\" : 'no-flickr_url', #str, \n",
    "                \"coco_url\" : url, #str, \n",
    "                \"date_captured\" : \"2018-20-01 00:00:00\", #datetime,\n",
    "            }\n",
    "\n",
    "            license = {\n",
    "                \"id\" : count_i, #int, \n",
    "                \"name\" : im_name, #str, \n",
    "                \"url\" : 'url', #str,\n",
    "            }\n",
    "\n",
    "            dataset['images'].append(image_dic)\n",
    "            dataset['licenses'].append(image_dic)\n",
    "\n",
    "            # parsing objects annotations\n",
    "            if isinstance(objects, dict): objects = [objects]\n",
    "        \n",
    "            for i in objects:\n",
    "                count_o += 1\n",
    "\n",
    "                o_cat_name = i['name'].lower()\n",
    "                if o_cat_name not in class_names: \n",
    "                    continue\n",
    "                o_cat_ix = cat_data.index(o_cat_name)\n",
    "                o_polygon = [[]]\n",
    "#                 o_polygon = i['polygon']\n",
    "#                 o_polygon = [adjust_polygon_format(o_polygon)]\n",
    "                o_bbox = [i['bndbox']['xmin'], i['bndbox']['ymin'], i['bndbox']['xmax'], i['bndbox']['ymax']]\n",
    "                o_bbox = xy_xy2ulxy_wh([int(i) for i in o_bbox])\n",
    "                try: o_dv_angle = i['direction']['angle']; total_dv_files += 1\n",
    "                except: o_dv_angle = None \n",
    "                \n",
    "                attbs = [o_cat_ix]\n",
    "                count_true_attbs = 0\n",
    "                for i, j in i['attributes'].items(): # i(attribute), j(list of Odicts or Odict)\n",
    "                    if isinstance(j, collections.OrderedDict): j = [j]\n",
    "                    for ODict in j: \n",
    "                        attb_name = ODict['@name'].lower()\n",
    "                        if ODict['#text'] == 'true' and attb_name in attbs_names: \n",
    "                            attbs.append(cat_data.index(attb_name))\n",
    "                            count_true_attbs += 1\n",
    "                \n",
    "                # overwrite to defaults if they all false\n",
    "                if o_cat_name == 'body' and count_true_attbs == 0:\n",
    "                    if default_attbs: \n",
    "                        assert default_attbs is not None\n",
    "                        default_anns +=1 \n",
    "                        [attbs.append(cat_data.index(i)) for i in default_attbs]                \n",
    "                        \n",
    "                if len(attbs) == 1: obss_wo_atts += 1\n",
    "                elif len(attbs) == 2: obss_w1_atts += 1\n",
    "                elif len(attbs) > 2: obss_ws_atts += 1\n",
    "                    \n",
    "                annotation = {\n",
    "                    \"id\" : count_o, #int, \n",
    "                    \"image_id\" : count_i, #int, \n",
    "                    \"category_id\" : attbs, # o_cat_ix, # attbs, #int, # list of ints\n",
    "                    \"segmentation\" : o_polygon, # polygon format or list \n",
    "                    \"area\" : 33., #float, \n",
    "                    \"bbox\" : o_bbox, #[x, y, width, height], \n",
    "                    \"iscrowd\" : 0, # 0 or 1,\n",
    "                    \"angle\" : float(o_dv_angle) if isinstance(o_dv_angle, str) else None, # float - directional vector - [-pi, pi],\n",
    "                }\n",
    "                \n",
    "                dataset['annotations'].append(annotation)\n",
    "    \n",
    "    print(obss_wo_atts, obss_w1_atts, obss_ws_atts, default_anns, count_not_anno)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "781 257 898 240 337\n",
      "1430 1936\n"
     ]
    }
   ],
   "source": [
    "total_size = len(ann_files)\n",
    "dataset = create_mcnn_dataset(ann_files, cat_file)# def_attbs_dic=default_attbs_dic)\n",
    "print(len(dataset['images']), len(dataset['annotations']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_file = '/home/blanca/mnt/data/__toy_plane/annotations/annotations_attbs_body_drill_1430.json'\n",
    "\n",
    "with open(out_file, 'w') as f:\n",
    "    json.dump(dataset, f, indent=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 969,\n",
       "  'width': 1920,\n",
       "  'height': 1080,\n",
       "  'file_name': '4_originals_000001.png',\n",
       "  'license': 0,\n",
       "  'flickr_url': 'no-flickr_url',\n",
       "  'coco_url': 'https://graphotate-dev.s3.eu-west-2.amazonaws.com/20181122170259338485/datasets/4/originals/00000001.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIQJH3IXHGCS6MBBA%2F20181122%2Feu-west-2%2Fs3%2Faws4_request&X-Amz-Date=20181122T172657Z&X-Amz-Expires=604800&X-Amz-Signature=86b7f87ed68fd67c43bf0c0cb201cb7f1de4d35e8c39dc4be89a6d4cf72f6b3c&X-Amz-SignedHeaders=host',\n",
       "  'date_captured': '2018-20-01 00:00:00'}]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[io for io in dataset['images'] if io['file_name'] == '4_originals_000001.png']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 572,\n",
       "  'image_id': 969,\n",
       "  'category_id': [0, 2, 4, 5, 6, 7],\n",
       "  'segmentation': [[]],\n",
       "  'area': 33.0,\n",
       "  'bbox': [148.0, 232.0, 771.0, 559.0],\n",
       "  'iscrowd': 0,\n",
       "  'angle': None}]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ao for ao in dataset['annotations'] if ao['image_id'] == 969]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Total images: 1430  |  Total saved images: 1430 \n"
     ]
    }
   ],
   "source": [
    "# move to utils\n",
    "import xmltodict\n",
    "from skimage import io\n",
    "from pathlib import Path\n",
    "import urllib\n",
    "\n",
    "def download_images_from_url_json(annotations_json_path, output_path, save_images=True): \n",
    "    \n",
    "    with open(annotations_json_path, 'r') as f: anns_dataset = json.load(f)\n",
    "    not_saved = 0\n",
    "    for imo in anns_dataset['images']:\n",
    "        iid, url = imo['id'], imo['coco_url']\n",
    "        try: im = np.asarray(bytearray(urllib.request.urlopen(url).read())) #, dtype=np.uint8)\n",
    "        except: \n",
    "            not_saved +=1; \n",
    "            continue\n",
    "        im = cv2.imdecode(im, -1)\n",
    "        \n",
    "        im_name = imo['file_name'] # .split('.png')[0] + '.jpg'\n",
    "        if save_images: cv2.imwrite(output_path + im_name, im)\n",
    "    \n",
    "    print(not_saved)\n",
    "    print('Total images: %s  |  Total saved images: %s ' %(len(anns_dataset['images']), len(list(Path(output_path).glob('*.png')))))\n",
    "        \n",
    "anns_file = '/home/blanca/mnt/data/__toy_plane/annotations/annotations_attbs_body_drill_1430.json'\n",
    "output_path = '/home/blanca/mnt/data/__toy_plane/JPEGImages_1430/'\n",
    "im = download_images_from_url_json(anns_file, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # move to utils\n",
    "# import xmltodict\n",
    "# from skimage import io\n",
    "# from pathlib import Path\n",
    "# import urllib\n",
    "\n",
    "# def download_images_from_url_json(annotations_json_path, output_path, save_images=True): \n",
    "    \n",
    "#     with open(annotations_json_path, 'r') as f: anns_dataset = json.load(f)\n",
    "    \n",
    "#     for imo in anns_dataset['images']:\n",
    "#         iid, url = imo['id'], imo['coco_url']        \n",
    "#         im = np.asarray(bytearray(urllib.request.urlopen(url).read()), dtype=np.uint8)\n",
    "#         im = cv2.imdecode(im, -1)\n",
    "#         im_name = imo['file_name']\n",
    "#         if save_images: cv2.imwrite(output_path + im_name, im)\n",
    "\n",
    "#     print('Total images: %s  |  Total saved images: %s ' %(len(anns_dataset['images']), len(list(Path(output_path).glob('*.png')))))\n",
    "        \n",
    "# anns_file = '/home/blanca/mnt/data/__toy_plane/annotations/annotations_attbs_body_drill.json'\n",
    "# output_path = '/home/blanca/mnt/data/__toy_plane/JPEGImages/'\n",
    "# im = download_images_from_url_json(anns_file, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(965, 1151)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anns_file = '/home/blanca/mnt/data/__toy_plane/annotations/annotations_attbs_body_drill.json'\n",
    "with open(anns_file, 'r') as f: anns_dataset = json.load(f)\n",
    "len(anns_dataset['images']), len(anns_dataset['annotations'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### filtering DV images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 0\n",
      "763 759 763\n",
      "0 0 0\n"
     ]
    }
   ],
   "source": [
    "# filtering images w/o dv\n",
    "dataset_dv = dataset.copy()\n",
    "images_list = [ao['image_id'] for ao in dataset_dv['annotations'] if isinstance(ao['angle'], float)]\n",
    "images_dv = [imo for imo in dataset_dv['images'] if imo['id'] in images_list]\n",
    "annotations_dv = [ao for ao in dataset_dv['annotations'] if isinstance(ao['angle'], float)]\n",
    "print(len(images_list), len(images_dv), len(annotations_dv))\n",
    "\n",
    "print(len(dataset_dv['annotations']), len(dataset_dv['images']), len(dataset_dv['annotations']))\n",
    "dataset_dv['images'] = images_dv\n",
    "dataset_dv['annotations'] = annotations_dv\n",
    "print(len(dataset_dv['annotations']), len(dataset_dv['images']), len(dataset_dv['annotations']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out_file = '/home/blanca/mnt/data/__toy_plane/annotations/annotations_attbs_body_drill_with_directions.json'\n",
    "\n",
    "# with open(out_file, 'w') as f:\n",
    "#     json.dump(dataset_dv, f, indent=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
